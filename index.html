<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html> <head> <link rel="icon" href="tficon/favicon.ico" type="image/x-icon"> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <title>TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition</title> <link href="style.css" rel="stylesheet"> <script type="text/javascript" src="tficon/demo.js"></script> </head> <body> <div class="content"> <h1>TF-ICON: Diffusion-Based Training-Free Cross-Domain <br> Image Composition</h1> <p id="authors"> Shilin Lu<sup>1</sup>    Yanzhu Liu<sup>2</sup>    Adams Wai-Kin Kong<sup>1</sup> <br> <span style="font-size: 16px"><sup>1</sup> School of Computer Science and Engineering, Nanyang Technological University, Singapore   <br> <sup>2</sup> Institute for Infocomm Research (I<sup>2</sup>R) &amp; Centre for Frontier AI Research (CFAR), A*STAR, Singapore </span></p> <br> <img src="./tficon/tf-icon.png" class="teaser-gif" style="width:100%;"><br> <font size="+2"> <p style="text-align: center;"> <a href="https://arxiv.org/abs/2307.12493" target="_blank" rel="external nofollow noopener">[Paper]</a>      <a href="https://github.com/Shilin-LU/TF-ICON/" target="_blank" rel="external nofollow noopener">[Code]</a>      <a href="https://entuedu-my.sharepoint.com/personal/shilin002_e_ntu_edu_sg/_layouts/15/onedrive.aspx?id=%2Fpersonal%2Fshilin002%5Fe%5Fntu%5Fedu%5Fsg%2FDocuments%2FTF%2DICON%20Test%20Benchmark&amp;ga=1/" target="_blank" rel="external nofollow noopener">[Benchmark]</a> </p> </font> </div> <div class="content"> <h2 style="text-align:center;">Abstract</h2> <p>Text-driven diffusion models have exhibited impressive generative capabilities, enabling various image editing tasks. In this paper, we propose TF-ICON, a novel Training-Free Image COmpositioN framework that harnesses the power of text-driven diffusion models for cross-domain image-guided composition. This task aims to seamlessly integrate user-provided objects into a specific visual context. Current diffusion-based methods often involve costly instance-based optimization or finetuning of pretrained models on customized datasets, which can potentially undermine their rich prior. In contrast, TF-ICON can leverage off-the-shelf diffusion models to perform cross-domain image-guided composition without requiring additional training, finetuning, or optimization. Moreover, we introduce the exceptional prompt, which contains no information, to facilitate text-driven diffusion models in accurately inverting real images into latent representations, forming the basis for compositing. Our experiments show that equipping Stable Diffusion with the exceptional prompt outperforms state-of-the-art inversion methods on various datasets (CelebA-HQ, COCO, and ImageNet), and that TF-ICON surpasses prior baselines in versatile visual domains.</p> </div> <div class="content"> <h2>TF-ICON: Cross-Domain Image Composition</h2> <p>Our framework facilitates the seamless compositing of subjects from distinct visual domains, leveraging user-provided images alongside the cutting-edge text-driven diffusion model, <a href="https://github.com/Stability-AI/stablediffusion" style="text-decoration: none; color: rgb(212, 65, 114);" rel="external nofollow noopener" target="_blank">Stable Diffusion</a>. Through our approach, we are able to effortlessly and precisely edit image contents while ensuring imperceptible transition areas for a truly seamless result. </p> <style>.cat-hat-button_a,.cat-hat-button_b{display:inline-flex;flex-direction:column;align-items:center;text-align:center}.cat-hat-button_a img,.cat-hat-button_b img{width:120px;height:120px}</style> <div class="cat-hat-main"> <img class="cat-hat-img" id="cat-hat-img" src="tficon\bg04_burger.png"> <div class="cat-hat-text"> <br><br><b>Scenes</b><br><br> <span class="cat-hat-bracket"> <span class="cat-hat-button_a"> <img src="tficon\bg04.png" alt="bg04"> photorealism </span>   <span class="cat-hat-button_a"> <img src="tficon\bg61.png" alt="bg61"> oil painting </span>   <span class="cat-hat-button_a"> <img src="tficon\bg50.png" alt="bg50"> sketch </span>   </span> <br><b>Objects</b><br><br> <span class="cat-hat-bracket"> <span class="cat-hat-button_b"> <img src="tficon\burger.png" alt="..."> burger </span>   <span class="cat-hat-button_b"> <img src="tficon\croissant.png" alt="floral"> croissant </span>   <span class="cat-hat-button_b"> <img src="tficon\muffin.png" alt="cylinder"> muffin </span>   </span> <br><br> </div> </div> </div> <div class="content"> <h2>Image Inversion with Exceptional Prompt</h2> <p> In the unconditional setting, solving the diffusion ODE in the reverse direction enables us to obtain the decent latent code for the real image. However, in the text-driven setting, the inversion process is prone to significant reconstruction errors, due to the instability induced by Classifier-Free Guidance (CFG). Our experiments further reveal that even without CFG, the unconditional output of text-driven diffusion models still produce large reconstruction errors. <br><br> <img class="summary-img" src="./tficon/inversion1_for_projpage.png" style="width:95%;"> <br> Two points require attention. Firstly, CFG typically amplifies instability, resulting in subpar inversion, while satisfactory reconstruction from CFG output is still possible, albeit less common. Secondly, the unconditional output does not necessarily outperform CFG or conditional one, as the unconditional/null prompt contains special symbols, which also add information and lead to inconsistent directional shifts in image embeddings. Thus, the unconditional output may perform poorly than others. Figure above shows unconditional (1st row), conditional (2nd row), or CFG (3rd row) output can yield the best reconstruction among them. <br> <br> The invertibility of diffusion models is provided by deterministic sampling. By modeling the diffusion process as a Stochastic Differential Equation (SDE), we can readily identify the associated Probability Flow Ordinary Differential Equation (PF ODE) that shares the same marginal probability density as the original SDE. The connection between their corresponding Partial Differential Equations (which describe the dynamics of their marginal distribution), namely the <a href="https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_equation" style="text-decoration: none; color: rgb(212, 65, 114);" rel="external nofollow noopener" target="_blank">Fokker-Planck Equation</a> and <a href="https://en.wikipedia.org/wiki/Continuity_equation" style="text-decoration: none; color: rgb(212, 65, 114);" rel="external nofollow noopener" target="_blank">Continuity Equation</a>, enables us to derive the PF ODE. The resulting PF ODE can be solved deterministically and with higher accuracy. <br> <br> The CFG disrupts the alignment between the forward and backward ODE trajectories, primarily due to the inclusion of additional information (please refer to <a href="./tficon/tficon.pdf" style="text-decoration: none; color: rgb(212, 65, 114);">Section A.2 of the paper</a> for more details). Intuitively, any information contained within the input prompt can result in the deviation of the backward ODE trajectories from the forward trajectories. To achieve accurate inversion, we present a straightforward yet effective solution, namely <i>exceptional prompt</i>. We remove all information by setting all token numbers to a common value and eliminating positional embeddings for the text prompt. Note that the exceptional prompt is distinguished from the null prompt by its absence of special tokens, such as <u>[startoftext]</u>, <u>[endoftext]</u>, and <u>[pad]</u>, which still retain information. The exceptional prompt is applied only in image inversion but not in the composition process. The choice of the token value does not significantly affect the inversion. <br><br> <img class="summary-img" src="./tficon/illegalprompt_for_proj.png" style="width:100%;"> </p> </div> <div class="content"> <h2>Image Composition</h2> <p> The main and reference images are inverted into noises by deterministically solving diffusion ODEs with the exceptional prompt. The accurate latent noises are then composed to form the starting point for the compositional generation. This involves the composition of three constituents for injection at early stage: self-attention maps stored from the main and reference image reconstruction processes, along with cross-attention between the main and reference images. For better clarity and readability, the original main and reference images are shown in the pixel space instead of the VAE latent space, and the reference image is presented without resizing and zero-padding. </p> <img class="summary-img" src="./tficon/framework_vector.png" style="width:100%;"> </div> <div class="content"> <h2>Qualitative Comparison</h2> <p> Qualitative comparison with SOTA and concurrent baselines in image-guided composition for sketching, photorealism, painting, and cartoon animation domains. </p> <img class="summary-img" src="./tficon/comparison1.png" style="width:100%;"> </div> <div class="content"> <h2>Ablation Study</h2> <img class="summary-img" src="./tficon/Additional_ablation.png" style="width:100%;"> </div> <div class="content"> <h4>BibTex</h4> <p> @InProceedings{lu2023tficon,<br>   title={TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition},<br>   author={Lu, Shilin and Liu, Yanzhu and Kong, Adams Wai-Kin},<br>   booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},<br>   year={2023}<br> } </p> </div> <div class="content" id="acknowledgements"> <p><strong>Acknowledgements</strong> </p> <p> The website template is borrowed from <a href="https://dreambooth.github.io/" style="text-decoration: none; color: rgb(212, 65, 114);" rel="external nofollow noopener" target="_blank">DreamBooth</a> and <a href="https://prompt-to-prompt.github.io/" style="text-decoration: none; color: rgb(212, 65, 114);" rel="external nofollow noopener" target="_blank">Prompt-to-Prompt</a>. </p> </div> </body> </html>